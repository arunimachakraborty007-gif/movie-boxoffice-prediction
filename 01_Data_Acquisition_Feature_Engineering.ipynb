{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üé¨ End-to-End Machine Learning: Box Office Prediction Pipeline\n",
        "## **Part 1: Data Acquisition & Feature Engineering**\n",
        "\n",
        "**Project:** Predicting the Opening Week Revenue for ***Avatar: Fire and Ash*** (Dec 2025).\n",
        "\n",
        "**Author:** Arunima Chakraborty\n",
        "\n",
        "**Date:** December 2025\n",
        "\n",
        "---\n",
        "\n",
        "### **Notebook Objective**\n",
        "This notebook serves as the **Data Engineering Engine** for the project. It connects to the **TMDb API**, scrapes 20+ years of high-revenue movie data, and engineers complex features (Star Power, Competition Scores, Inflation Adjustment) to prepare a dataset for the regression models in Part 2.\n",
        "\n",
        "### **Pipeline Overview**\n",
        "1.  **Phase 0: Setup** - Library installation and API authentication.\n",
        "2.  **Phase 1: Target Acquisition** - Scrapes 20,000 movies using time-chunking to bypass API limits.\n",
        "3.  **Phase 2: Metadata Enrichment** - Queries granular details (Budget, Cast, Keywords) and constructs the target variable proxy.\n",
        "4.  **Phase 3: Feature Engineering** - Applies historical tracking for Star Power and implements inflation adjustment (CPI).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "08hhlGXh5D_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hide warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1. Install Libraries\n",
        "!pip install tmdbv3api cpi pandas numpy scikit-learn seaborn matplotlib shap xgboost requests beautifulsoup4 fake-useragent\n",
        "\n",
        "# 2. Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cpi\n",
        "import time\n",
        "import ast\n",
        "import itertools\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta, date\n",
        "from tmdbv3api import TMDb, Find, Movie, Discover\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# 3. User Configuration\n",
        "API_KEY = \"2408ba3105fdbab4b16bb458113ed558\"  # <--- PASTE YOUR KEY HERE\n",
        "START_YEAR = 2000\n",
        "END_YEAR = 2025\n",
        "CURRENT_YEAR = 2025\n",
        "\n",
        "# 4. Setup API\n",
        "tmdb = TMDb()\n",
        "tmdb.api_key = API_KEY\n",
        "tmdb_movie = Movie()\n",
        "discover = Discover()\n",
        "\n",
        "# 5. Suppress CPI Warnings\n",
        "import logging\n",
        "logging.getLogger(\"cpi\").setLevel(logging.ERROR)\n",
        "try: cpi.update()\n",
        "except: pass\n",
        "\n",
        "print(\"Phase 0: Setup Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh7If58ybAjK",
        "outputId": "07232890-20a3-409c-9695-0a72b71b794b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tmdbv3api\n",
            "  Downloading tmdbv3api-1.9.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting cpi\n",
            "  Downloading cpi-2.0.8-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from cpi) (8.3.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from cpi) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.28.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->cpi) (1.17.0)\n",
            "Downloading tmdbv3api-1.9.0-py3-none-any.whl (25 kB)\n",
            "Downloading cpi-2.0.8-py2.py3-none-any.whl (18.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fake-useragent, tmdbv3api, cpi\n",
            "Successfully installed cpi-2.0.8 fake-useragent-2.2.0 tmdbv3api-1.9.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cpi:CPI data is out of date. To accurately inflate to today's dollars, you must run `cpi.update()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 0: Setup Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PHASE 1: Target Acquisition**\n",
        "**Goal:** Scrape 20,000 high-revenue movies using Time Chunking to bypass API limits."
      ],
      "metadata": {
        "id": "y3SMZ1J1ctDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Phase 1: Target Acquisition (Fresh Start)...\")\n",
        "\n",
        "movies_fallback = []\n",
        "target_count = 20000\n",
        "\n",
        "# Split into chunks to bypass 10k limit\n",
        "time_chunks = [\n",
        "    (START_YEAR, START_YEAR + 12),      # 2000-2012\n",
        "    (START_YEAR + 13, END_YEAR)         # 2013-2025\n",
        "]\n",
        "\n",
        "for start_chunk, end_chunk in time_chunks:\n",
        "    print(f\"Fetching movies for years {start_chunk}-{end_chunk}...\")\n",
        "    page = 1\n",
        "\n",
        "    while page <= 500 and len(movies_fallback) < target_count:\n",
        "        try:\n",
        "            disc = discover.discover_movies({\n",
        "                'sort_by': 'revenue.desc',\n",
        "                'primary_release_date.gte': f'{start_chunk}-01-01',\n",
        "                'primary_release_date.lte': f'{end_chunk}-12-31',\n",
        "                'with_original_language': 'en',\n",
        "                'page': page\n",
        "            })\n",
        "\n",
        "            if not disc: break\n",
        "\n",
        "            for m in disc:\n",
        "                if not any(x['tmdb_id'] == m['id'] for x in movies_fallback):\n",
        "                    movies_fallback.append({\n",
        "                        'tmdb_id': m['id'],\n",
        "                        'release_date': m.get('release_date')\n",
        "                    })\n",
        "\n",
        "            if len(movies_fallback) % 1000 == 0:\n",
        "                print(f\"  Progress: {len(movies_fallback)} movies found...\")\n",
        "\n",
        "            page += 1\n",
        "            time.sleep(0.1)\n",
        "        except: break\n",
        "\n",
        "target_df = pd.DataFrame(movies_fallback)\n",
        "target_df.to_csv('phase1_target_list.csv', index=False)\n",
        "print(f\"\\nPhase 1 Complete. {len(target_df)} movies ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU0rPPL5bQ7s",
        "outputId": "4ebb8a12-f76a-4273-aa0e-86afff97e826"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 1: Target Acquisition (Fresh Start)...\n",
            "Fetching movies for years 2000-2012...\n",
            "  Progress: 1000 movies found...\n",
            "  Progress: 2000 movies found...\n",
            "  Progress: 3000 movies found...\n",
            "Fetching movies for years 2013-2025...\n",
            "\n",
            "Phase 1 Complete. 19170 movies ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Feature Engineering**\n",
        "**Goal:** Fetch Metadata.\n",
        "\n",
        "**Fix:** Saves tmdb_title so we can see movie names later."
      ],
      "metadata": {
        "id": "TD64vilieFQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this phase, we iterate through our filtered movie list and query the **The Movie Database (TMDb) API** for granular metadata. This step addresses the \"Sparse Data\" problem by engineering high-value features and normalizing financial metrics.\n",
        "\n",
        "**Key Methodological Actions:**\n",
        "1.  **Target Variable Construction (The \"Proxy\"):**\n",
        "    * *Problem:* TMDb provides \"Total Global Revenue\" but rarely splits out \"Domestic Opening Week.\"\n",
        "    * *Solution:* We engineered a proxy target using industry-standard decay curves:\n",
        "        `Target = Global Revenue √ó 0.40 (Domestic Share) √ó 0.35 (First Week Share)`.\n",
        "    * *Justification:* This standardizes the target variable, allowing the model to learn opening-week dynamics even when explicit data is missing.\n",
        "\n",
        "2.  **Economic Normalization (CPI Adjustment):**\n",
        "    * We integrated the **CPI (Consumer Price Index)** library to adjust all historical Budgets and Revenues to **2025 Dollars**. This ensures that a \\$100M budget in 2009 (*Avatar*) is mathematically comparable to a \\$400M budget in 2025.\n",
        "\n",
        "3.  **Noise Filtering:**\n",
        "    * We implemented a \"Garbage Filter\" to drop entries with `< $1,000` in budget or revenue, removing student films and data entry errors that could skew the regression gradients.\n",
        "\n",
        "4.  **Metadata Extraction:**\n",
        "    * **MPAA Rating:** Encoded as ordinal data (G=0 to NC-17=4).\n",
        "    * **Talent IDs:** Extracted Director and Top 3 Cast IDs to build the \"Star Power\" history in Phase 3.\n",
        "    * **Franchise Status:** Boolean flag for sequels (`belongs_to_collection`)."
      ],
      "metadata": {
        "id": "QItX_orI3M5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Phase 2: Feature Engineering...\")\n",
        "\n",
        "final_data = []\n",
        "skip_low_data = 0\n",
        "\n",
        "def get_mpaa(releases):\n",
        "    if not hasattr(releases, 'results'): return 2\n",
        "    for r in releases.results:\n",
        "        if r['iso_3166_1'] == 'US':\n",
        "            for d in r['release_dates']:\n",
        "                if d['certification']:\n",
        "                    return {'G':0, 'PG':1, 'PG-13':2, 'R':3, 'NC-17':4}.get(d['certification'], 2)\n",
        "    return 2\n",
        "\n",
        "# Iterate through target list\n",
        "for i, row in target_df.iterrows():\n",
        "    try:\n",
        "        m_id = row['tmdb_id']\n",
        "        details = tmdb_movie.details(m_id, append_to_response='credits,keywords,release_dates')\n",
        "\n",
        "        budget = getattr(details, 'budget', 0)\n",
        "        revenue = getattr(details, 'revenue', 0)\n",
        "\n",
        "        # Filter \"Garbage\" (Low Budget/Revenue)\n",
        "        if budget < 1000 or revenue < 1000:\n",
        "            skip_low_data += 1\n",
        "            continue\n",
        "\n",
        "        # Target Calculation (Proxy for Opening Weekend)\n",
        "        # Global * 40% (Domestic Share) * 35% (First Week Share)\n",
        "        target_val = revenue * 0.40 * 0.35\n",
        "\n",
        "        # Feature Extraction\n",
        "        keywords = [k['name'].lower() for k in list(details.keywords.keywords)] if hasattr(details, 'keywords') else []\n",
        "\n",
        "        top_cast = []\n",
        "        director = None\n",
        "        if hasattr(details, 'credits'):\n",
        "            if hasattr(details.credits, 'cast'):\n",
        "                top_cast = [c['id'] for c in list(details.credits.cast)[:3]]\n",
        "            if hasattr(details.credits, 'crew'):\n",
        "                director = next((c['id'] for c in list(details.credits.crew) if c['job'] == 'Director'), None)\n",
        "\n",
        "        is_sequel = 1 if hasattr(details, 'belongs_to_collection') and details.belongs_to_collection else 0\n",
        "        coll_id = details.belongs_to_collection['id'] if is_sequel else None\n",
        "\n",
        "        # Inflation Adjustment\n",
        "        year = pd.to_datetime(details.release_date).year\n",
        "        try: inf_factor = cpi.inflate(1.0, year, to=CURRENT_YEAR)\n",
        "        except: inf_factor = 1.0\n",
        "\n",
        "        final_data.append({\n",
        "            'tmdb_title': details.title,\n",
        "            'release_date': pd.to_datetime(details.release_date),\n",
        "            'Budget_Adj': budget * inf_factor,\n",
        "            'Domestic_First_Week_Adj': target_val * inf_factor,\n",
        "            'Runtime': getattr(details, 'runtime', 0),\n",
        "            'MPAA_Rating': get_mpaa(getattr(details, 'release_dates', None)),\n",
        "            'Director_ID': director,\n",
        "            'Top_Cast_IDs': top_cast,\n",
        "            'Keywords': keywords,\n",
        "            'Is_Sequel': is_sequel,\n",
        "            'Collection_ID': coll_id,\n",
        "            'Distributor_List': list(getattr(details, 'production_companies', []))\n",
        "        })\n",
        "\n",
        "    except: pass\n",
        "\n",
        "    # Progress & Rate Limiting\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Processed {i}/{len(target_df)}... (Saved: {len(final_data)})\")\n",
        "    time.sleep(0.08)\n",
        "\n",
        "df = pd.DataFrame(final_data)\n",
        "df.to_csv('phase2_features.csv', index=False)\n",
        "print(f\"Phase 2 Complete. {len(df)} movies saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBvFM0SGcUa5",
        "outputId": "0137a4f5-3509-4aef-b07c-af3595332683"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 2: Feature Engineering...\n",
            "Processed 0/19170... (Saved: 1)\n",
            "Processed 100/19170... (Saved: 101)\n",
            "Processed 200/19170... (Saved: 201)\n",
            "Processed 300/19170... (Saved: 301)\n",
            "Processed 400/19170... (Saved: 401)\n",
            "Processed 500/19170... (Saved: 500)\n",
            "Processed 600/19170... (Saved: 600)\n",
            "Processed 700/19170... (Saved: 699)\n",
            "Processed 800/19170... (Saved: 798)\n",
            "Processed 900/19170... (Saved: 897)\n",
            "Processed 1000/19170... (Saved: 995)\n",
            "Processed 1100/19170... (Saved: 1091)\n",
            "Processed 1200/19170... (Saved: 1189)\n",
            "Processed 1300/19170... (Saved: 1284)\n",
            "Processed 1400/19170... (Saved: 1382)\n",
            "Processed 1500/19170... (Saved: 1478)\n",
            "Processed 1600/19170... (Saved: 1573)\n",
            "Processed 1700/19170... (Saved: 1663)\n",
            "Processed 1800/19170... (Saved: 1752)\n",
            "Processed 1900/19170... (Saved: 1840)\n",
            "Processed 2000/19170... (Saved: 1925)\n",
            "Processed 2100/19170... (Saved: 2008)\n",
            "Processed 2200/19170... (Saved: 2088)\n",
            "Processed 2300/19170... (Saved: 2164)\n",
            "Processed 2500/19170... (Saved: 2301)\n",
            "Processed 2800/19170... (Saved: 2486)\n",
            "Processed 2900/19170... (Saved: 2538)\n",
            "Processed 3000/19170... (Saved: 2594)\n",
            "Processed 3200/19170... (Saved: 2681)\n",
            "Processed 3300/19170... (Saved: 2736)\n",
            "Processed 3800/19170... (Saved: 2924)\n",
            "Processed 9200/19170... (Saved: 2986)\n",
            "Processed 9300/19170... (Saved: 3086)\n",
            "Processed 9400/19170... (Saved: 3185)\n",
            "Processed 9500/19170... (Saved: 3285)\n",
            "Processed 9600/19170... (Saved: 3383)\n",
            "Processed 9700/19170... (Saved: 3482)\n",
            "Processed 9800/19170... (Saved: 3581)\n",
            "Processed 9900/19170... (Saved: 3677)\n",
            "Processed 10000/19170... (Saved: 3775)\n",
            "Processed 10100/19170... (Saved: 3870)\n",
            "Processed 10200/19170... (Saved: 3967)\n",
            "Processed 10400/19170... (Saved: 4152)\n",
            "Processed 10500/19170... (Saved: 4241)\n",
            "Processed 10600/19170... (Saved: 4333)\n",
            "Processed 10900/19170... (Saved: 4562)\n",
            "Processed 11000/19170... (Saved: 4632)\n",
            "Processed 11500/19170... (Saved: 4895)\n",
            "Processed 11700/19170... (Saved: 4973)\n",
            "Processed 11900/19170... (Saved: 5035)\n",
            "Processed 12500/19170... (Saved: 5239)\n",
            "Processed 12900/19170... (Saved: 5376)\n",
            "Phase 2 Complete. 5649 movies saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following phase, we transform the raw metadata into quantifiable features suitable for machine learning. This involves both static processing and dynamic historical tracking.\n",
        "\n",
        "**Key Engineering Steps:**\n",
        "1.  **Distributor Tiering:** We categorize production companies into Tiers (1=Major Studios like Disney/Warner, 2=Mid-Majors, 3=Indies) to serve as a proxy for marketing power and distribution reach.\n",
        "2.  **Theater Count Imputation:** Since historical theater counts are often missing, we estimate them based on the Distributor Tier (e.g., Tier 1 releases typically secure ~4,000+ screens).\n",
        "3.  **Genre One-Hot Encoding:** We explicitly flag high-value genres (Sci-Fi, Adventure, Action) and premium formats (IMAX/3D) derived from keyword tags.\n",
        "4.  **\"Time-Travel Safe\" History:** We implement a rolling loop that iterates through movies chronologically to calculate cumulative statistics without data leakage:\n",
        "    * **Star Power:** Sum of inflation-adjusted gross revenue generated by the director and top cast in *previous* movies.\n",
        "    * **Actor Familiarity:** A graph-based metric counting how often the cast members have worked together before.\n",
        "    * **Competition Score:** A count of other major releases launching within a ¬±7 day window.\n",
        "    * **Sequel Performance:** Retrieves the adjusted gross of the immediate predecessor if the film is part of a franchise."
      ],
      "metadata": {
        "id": "tTErnOsy2J3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Phase 3: Cleaning & History...\")\n",
        "\n",
        "if 'df' in locals() and not df.empty:\n",
        "    df = df.sort_values('release_date').reset_index(drop=True)\n",
        "\n",
        "    # 1. Distributor Processing\n",
        "    def get_dist_info(comps):\n",
        "        names = []\n",
        "        if isinstance(comps, list):\n",
        "            for c in comps:\n",
        "                if hasattr(c, 'name'): names.append(c.name)\n",
        "                elif isinstance(c, dict) and 'name' in c: names.append(c['name'])\n",
        "\n",
        "        primary = names[0] if names else \"Unknown\"\n",
        "        tier = 3\n",
        "        lower = [n.lower() for n in names]\n",
        "        if any(k in n for n in lower for k in ['disney', 'warner', 'universal', 'paramount', 'sony', 'marvel', 'pixar']): tier = 1\n",
        "        elif any(k in n for n in lower for k in ['lionsgate', 'a24', 'netflix', 'dreamworks']): tier = 2\n",
        "        return pd.Series([primary, tier])\n",
        "\n",
        "    df[['Distributor_Name', 'Distributor_Tier']] = df['Distributor_List'].apply(get_dist_info)\n",
        "\n",
        "    # 2. Impute Theater Count\n",
        "    def estimate_screens(row):\n",
        "        if row['Distributor_Tier'] == 1: return 4000\n",
        "        if row['Distributor_Tier'] == 2: return 2500\n",
        "        return 500\n",
        "    df['Theater_Count'] = df.apply(estimate_screens, axis=1)\n",
        "\n",
        "    # 3. Enhanced Genre Engineering\n",
        "    def check_genre(k_list, targets):\n",
        "        if isinstance(k_list, list): return 1 if any(t in k_list for t in targets) else 0\n",
        "        return 0\n",
        "\n",
        "    df['Genre_SciFi'] = df['Keywords'].apply(lambda x: check_genre(x, ['sci-fi', 'science fiction', 'alien', 'future', 'space']))\n",
        "    df['Genre_Adventure'] = df['Keywords'].apply(lambda x: check_genre(x, ['adventure', 'journey', 'quest']))\n",
        "    df['Genre_Action'] = df['Keywords'].apply(lambda x: check_genre(x, ['action', 'war', 'battle', 'fight']))\n",
        "    df['Genre_Fantasy'] = df['Keywords'].apply(lambda x: check_genre(x, ['fantasy', 'magic', 'myth', 'creature', 'monster']))\n",
        "    df['Has_IMAX_3D'] = df['Keywords'].apply(lambda x: check_genre(x, ['3d', 'imax']))\n",
        "    df['Release_Month'] = df['release_date'].dt.month\n",
        "\n",
        "    # 4. Rolling History Loop (Time Travel Safe)\n",
        "    dir_hist, cast_hist, collab_graph = {}, {}, {}\n",
        "    c_dir, c_cast, c_fam, c_prev, c_comp = [], [], [], [], []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Competition Score\n",
        "        start, end = row['release_date'] - timedelta(days=7), row['release_date'] + timedelta(days=7)\n",
        "        c_comp.append(max(0, df[(df['release_date'] >= start) & (df['release_date'] <= end)].shape[0] - 1))\n",
        "\n",
        "        # Sequel History\n",
        "        prev = 0\n",
        "        if row['Is_Sequel'] and row['Collection_ID']:\n",
        "            sibs = df[(df['Collection_ID'] == row['Collection_ID']) & (df['release_date'] < row['release_date'])]\n",
        "            if not sibs.empty: prev = sibs.iloc[-1]['Domestic_First_Week_Adj']\n",
        "        c_prev.append(prev)\n",
        "\n",
        "        # Star Power\n",
        "        c_dir.append(dir_hist.get(row['Director_ID'], 0))\n",
        "        c_cast.append(sum([cast_hist.get(a, 0) for a in row['Top_Cast_IDs']]))\n",
        "\n",
        "        # Familiarity\n",
        "        fam = 0\n",
        "        if len(row['Top_Cast_IDs']) > 1:\n",
        "            for pair in itertools.combinations(row['Top_Cast_IDs'], 2):\n",
        "                fam += collab_graph.get(frozenset(pair), 0)\n",
        "        c_fam.append(fam)\n",
        "\n",
        "        # Update Trackers\n",
        "        rev = row['Domestic_First_Week_Adj']\n",
        "        if row['Director_ID']: dir_hist[row['Director_ID']] = dir_hist.get(row['Director_ID'], 0) + rev\n",
        "        for a in row['Top_Cast_IDs']: cast_hist[a] = cast_hist.get(a, 0) + rev\n",
        "        if len(row['Top_Cast_IDs']) > 1:\n",
        "            for pair in itertools.combinations(row['Top_Cast_IDs'], 2):\n",
        "                key = frozenset(pair)\n",
        "                collab_graph[key] = collab_graph.get(key, 0) + 1\n",
        "\n",
        "    df['Competition_Score'] = c_comp\n",
        "    df['Prev_Movie_Gross_Adj'] = c_prev\n",
        "    df['Director_Prev_Gross'] = c_dir\n",
        "    df['Cast_Star_Power'] = c_cast\n",
        "    df['Actor_Familiarity'] = c_fam\n",
        "\n",
        "    # 5. Imputation & Save\n",
        "    mask = (df['Is_Sequel'] == 1) & (df['Prev_Movie_Gross_Adj'] == 0)\n",
        "    if mask.sum() > 0:\n",
        "        ratio = (df['Prev_Movie_Gross_Adj'] / df['Budget_Adj']).median()\n",
        "        df.loc[mask, 'Prev_Movie_Gross_Adj'] = df.loc[mask, 'Budget_Adj'] * (ratio if not np.isnan(ratio) else 3.0)\n",
        "\n",
        "    df.to_csv('Avatar_Final_Dataset_Enhanced.csv', index=False)\n",
        "    print(\"Phase 3 Complete. Dataset Saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT2gsjK5gAA5",
        "outputId": "dae4f4aa-eb79-430a-9fb8-be0db4188e39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 3: Cleaning & History...\n",
            "Phase 3 Complete. Dataset Saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# DOWNLOAD THE DATASET\n",
        "# ==========================================\n",
        "from google.colab import files\n",
        "\n",
        "try:\n",
        "    print(\"Downloading 'Avatar_Final_Dataset_Enhanced.csv'...\")\n",
        "    files.download('Avatar_Final_Dataset_Enhanced.csv')\n",
        "except ImportError:\n",
        "    print(\"Error: This download code only works in Google Colab.\")\n",
        "    print(\"If you are running locally, the file is already in your folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "UeKYY9NFd9IE",
        "outputId": "185e33d6-652f-4aa0-db03-85515e160791"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'Avatar_Final_Dataset_Enhanced.csv'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d28d7f73-0049-482a-a6ee-babf07d09bb3\", \"Avatar_Final_Dataset_Enhanced.csv\", 3900574)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Status:** Data Pipeline Complete.\n",
        "\n",
        "**Next Step:** Proceeding to Notebook `02_EDA_Modeling_and_Final_Prediction.ipynb` for EDA, Modeling, and the Final Prediction."
      ],
      "metadata": {
        "id": "F_mtPM3I6Pld"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNNY9AvX7Mr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}